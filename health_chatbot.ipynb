{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accc82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Shape: (47603, 4)\n",
      "Validation Dataset Shape: (11901, 4)\n",
      "\n",
      "Sample from Training Data:\n",
      "                                      short_question  \\\n",
      "0  can an antibiotic through an iv give you a ras...   \n",
      "1  can you test positive from having the hep b va...   \n",
      "2  what are the dietary restrictions for celiac d...   \n",
      "3  can i transmit genital warts seventeen years a...   \n",
      "4                          is all vitamin d the same   \n",
      "\n",
      "                                        short_answer                   tags  \\\n",
      "0  yes it can even after you have finished the pr...  ['rash' 'antibiotic']   \n",
      "1  test positive for what if you had a hep b vacc...        ['hepatitis b']   \n",
      "2  omitting gluten from the diet is the key to co...     ['celiac disease']   \n",
      "3  famotidine pepcid products is in a drug class ...               ['wart']   \n",
      "4  hi this means you do not have hepatitis b and ...          ['vitamin d']   \n",
      "\n",
      "   label  \n",
      "0    1.0  \n",
      "1    1.0  \n",
      "2    1.0  \n",
      "3   -1.0  \n",
      "4   -1.0  \n"
     ]
    }
   ],
   "source": [
    "# ðŸ§ª 1. Load and Explore Data\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"F:\\\\ML Projects\\\\health_chatbot\\\\data\\\\train_data_chatbot.csv\")\n",
    "val_df = pd.read_csv(\"F:\\\\ML Projects\\\\health_chatbot\\\\data\\\\validation_data_chatbot.csv\")\n",
    "\n",
    "print(\"Train Dataset Shape:\", train_df.shape)\n",
    "print(\"Validation Dataset Shape:\", val_df.shape)\n",
    "print(\"\\nSample from Training Data:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f487990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hafiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hafiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hafiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§¼ 2. Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca07066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ 3. Apply Preprocessing\n",
    "train_df['clean_question'] = train_df['short_question'].apply(preprocess_text)\n",
    "train_df['clean_answer'] = train_df['short_answer'].apply(preprocess_text)\n",
    "\n",
    "val_df['clean_question'] = val_df['short_question'].apply(preprocess_text)\n",
    "val_df['clean_answer'] = val_df['short_answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e58fa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0008402655239055541\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§  4. TF-IDF Similarity-based Answer Retrieval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_train = vectorizer.fit_transform(train_df['clean_question'])\n",
    "tfidf_val = vectorizer.transform(val_df['clean_question'])\n",
    "\n",
    "predicted_answers = []\n",
    "for i in range(tfidf_val.shape[0]):\n",
    "    sim_scores = cosine_similarity(tfidf_val[i], tfidf_train)[0]\n",
    "    best_match_idx = np.argmax(sim_scores)\n",
    "    predicted_answers.append(train_df.iloc[best_match_idx]['clean_answer'])\n",
    "\n",
    "print('Accuracy:', accuracy_score(val_df['clean_answer'], predicted_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796c40ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafiz\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\hafiz\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hafiz\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# 5. Encode Questions with Sentence Embeddings\n",
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained model (can be replaced with more specific ones like 'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings for all training questions\n",
    "question_embeddings = model.encode(train_df['clean_question'].tolist(), convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4145b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define the search function\n",
    "\n",
    "def get_answer(user_question, question_embeddings, questions, answers, model, top_k=1):\n",
    "    # Encode the user query\n",
    "    query_embedding = model.encode(user_question, convert_to_tensor=True)\n",
    "\n",
    "    # Semantic search\n",
    "    hits = util.semantic_search(query_embedding, question_embeddings, top_k=top_k)[0]\n",
    "\n",
    "    # Return best-matching answer\n",
    "    top_hit = hits[0]\n",
    "    return answers[top_hit['corpus_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d9b071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: may see one two drop normal see two drop may received full dose inject another dose talk healthcare provider prevent dripping leaking sure firmly push hold knob thumb 10 second removing needle skin\n"
     ]
    }
   ],
   "source": [
    "# 7. Test Chatbot Locally\n",
    "\n",
    "user_question = \"What are the symptoms of diabetes?\"\n",
    "response = get_answer(user_question, question_embeddings, train_df['clean_question'], train_df['clean_answer'], model)\n",
    "print(\"Bot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
